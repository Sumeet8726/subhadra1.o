
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Robotic Face</title>
  <style>
    body {
      margin: 0;
      height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
      background: linear-gradient(135deg, #07003d, #07003d);
      color: white;
      overflow: hidden;
    }

    .robot {
      position: relative;
      width: 300px;
      height: 400px;
      border-radius: 20px;
      display: flex;
      flex-direction: column;
      background: #07003d;
      align-items: center;
      justify-content: center;
      overflow: hidden;
    }

    .eyes {
      display: flex;
      justify-content: space-between;
      width: 60%;
      margin-bottom: 50px;
    }

    .eye {
      position: relative;
      width: 50px;
      height: 50px;
      background: radial-gradient(circle, rgb(255, 0, 191), rgb(85, 0, 78));
      border-radius: 50%;
      overflow: hidden;
      box-shadow: 0 0 15px rgb(255, 0, 191);
    }

    .pupil {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      width: 20px;
      height: 20px;
      background: #fff;
      border-radius: 50%;
      animation: eye-move 3s infinite;
    }

    @keyframes eye-move {
      0%, 100% {
        transform: translate(-50%, -50%);
      }
      25% {
        transform: translate(-70%, -50%);
      }
      50% {
        transform: translate(-50%, -70%);
      }
      75% {
        transform: translate(-30%, -50%);
      }
    }

    .mouth {
      position: relative;
      width: 120px;
      height: 50px;
      background: linear-gradient(to bottom, #07003d, #07003d);
      border-radius: 10px;
      overflow: hidden;
      animation: mouth-move 2s infinite;
    }

    .mouth::after {
      content: '';
      position: absolute;
      width: 100%;
      height: 100%;
      background: repeating-linear-gradient(90deg, rgb(255, 0, 191), rgb(255, 0, 191) 10px, #07003d 10px, #07003d 20px);
      animation: mouth-glow 2s infinite linear;
    }

    @keyframes mouth-move {
      0%, 100% {
        transform: translateY(0);
      }
      50% {
        transform: translateY(-10px);
      }
    }

    @keyframes mouth-glow {
      from {
        background-position: 0;
      }
      to {
        background-position: 40px;
      }
    }

    .background {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: #333
      z-index: -1;
    }
  </style>
</head>
<body>
  <div class="robot">
    <div class="eyes">
      <div class="eye">
        <div class="pupil"></div>
      </div>
      <div class="eye">
        <div class="pupil"></div>
      </div>
    </div>
    <div class="mouth"></div>
  </div>
  <div class="background"></div>
  <div id="status" style="position: fixed; bottom: 1px; right: 5px; font-size: 10px; color: white;"></div>
  <button id="startVoice" style="position: fixed; bottom: 10px; right: 10px; cursor: pointer; background: #0ff; padding: 10px; border-radius: 50%; box-shadow: 0 0 10px #0ff;">
    ðŸŽ¤
  </button>
  <p id="conversation"></p>
  <script>
    // Initialize SpeechRecognition
    const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
    const synth = window.speechSynthesis;
    const API_KEY = "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"; // Replace with your API key
    const API_URL = "https://api-inference.huggingface.co/models/facebook/blenderbot-400M-distill"; // Correct model endpoint
    const statusElement = document.getElementById("status");
    const startBtn = document.getElementById("startVoice");
    const responseDiv = document.getElementById("conversation");

    // Ensure voices are loaded before use
    let femaleVoice = null;

    const loadVoices = () => {
        const voices = synth.getVoices();
        femaleVoice = voices.find(voice => 
            voice.gender === "female" || 
            voice.name.toLowerCase().includes("female") || 
            voice.name.toLowerCase().includes("woman")
        );
        
        // Log a warning if no female voice is found
        if (!femaleVoice) {
            console.warn("No female voice found; default voice will be used.");
        }
    };

    // Wait for voices to load
    if (synth.onvoiceschanged !== undefined) {
        synth.onvoiceschanged = loadVoices;
    } else {
        loadVoices(); // Load immediately if voices are already available
    }

    startBtn.addEventListener("click", () => {
        statusElement.textContent = "Listening...";
        recognition.start();
    });

    recognition.onresult = async (event) => {
        const userInput = event.results[0][0].transcript;
        statusElement.textContent = `You said: ${userInput}`;

        // Send the user's speech to the Hugging Face Blenderbot API
        const botResponse = await fetch(API_URL, {
            method: "POST",
            headers: {
                "Content-Type": "application/json",
                Authorization: `Bearer ${API_KEY}`,
            },
            body: JSON.stringify({
                inputs: userInput, // Correct input structure for Blenderbot
            }),
        })
            .then((res) => res.json())
            .then((data) => data.generated_text || "Sorry, I couldn't understand that.")
            .catch((err) => {
                console.error("API Error:", err);
                return "I'm sorry, I couldn't process that.";
            });

        // Display and speak the AI's response
        speak(botResponse);
        responseDiv.textContent = `Bot: ${botResponse}`;
    };

    recognition.onerror = (event) => {
        statusElement.textContent = `Error: ${event.error}`;
        speak("Sorry, I couldn't understand that.");
    };

    // Universal Speak Function
    const speak = (text) => {
        const utterance = new SpeechSynthesisUtterance(text);
        if (femaleVoice) {
            utterance.voice = femaleVoice; // Assign the selected female voice
        } else {
            console.warn("Using default voice for speech.");
        }
        synth.speak(utterance);
        utterance.onend = () => {
            statusElement.textContent = "";
        };
    };

  </script>
</body>
</html>
